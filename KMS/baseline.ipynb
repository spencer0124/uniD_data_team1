{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import CenterCrop, Resize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':15,\n",
    "    'LEARNING_RATE':0.0002086672211449482,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42,\n",
    "    'HIDDEN_UNITS': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(base_dir):\n",
    "    clean_dir = os.path.join(base_dir, 'clean')\n",
    "    noisy_dir = os.path.join(base_dir, 'noisy')\n",
    "\n",
    "    os.makedirs(clean_dir, exist_ok=True)\n",
    "    os.makedirs(noisy_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    source_dirs = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if 'GT' in dir_name:\n",
    "                source_dirs.append(os.path.join(root, dir_name))\n",
    "\n",
    "    if not source_dirs:\n",
    "        raise ValueError(\"No directory containing 'GT' found\")\n",
    "\n",
    "    for source_dir in source_dirs:\n",
    "        for filename in os.listdir(source_dir):\n",
    "            if filename.endswith('.jpg'):\n",
    "                shutil.move(os.path.join(source_dir, filename), os.path.join(clean_dir, filename))\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name not in ['clean', 'noisy'] and 'GT' not in dir_name:\n",
    "                current_dir = os.path.join(root, dir_name)\n",
    "                for filename in os.listdir(current_dir):\n",
    "                    if filename.endswith('.jpg'):\n",
    "                        shutil.move(os.path.join(current_dir, filename), os.path.join(noisy_dir, filename))\n",
    "                        \n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            if dir_name not in ['clean', 'noisy']:\n",
    "                shutil.rmtree(dir_path)\n",
    "                        \n",
    "    print('preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No directory containing 'GT' found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_393953/3841326738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_base_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_base_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_base_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_393953/699968921.py\u001b[0m in \u001b[0;36mPreprocess\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No directory containing 'GT' found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msource_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No directory containing 'GT' found"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "training_base_dir = os.path.join(data_dir, 'Training')\n",
    "validation_base_dir = os.path.join(data_dir, 'Validation')\n",
    "\n",
    "Preprocess(training_base_dir)\n",
    "Preprocess(validation_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "        \n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return noisy_clean_pairs\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform(clean_image)\n",
    "        \n",
    "        return noisy_image, clean_image\n",
    "    \n",
    "\n",
    "class CutMixDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, beta=1.0, prob=0.5):\n",
    "        self.dataset = dataset\n",
    "        self.beta = beta\n",
    "        self.prob = prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def rand_bbox(self, size, lam):\n",
    "        if len(size) == 3:  # (C, H, W) 형식인 경우\n",
    "            W = size[1]\n",
    "            H = size[2]\n",
    "        elif len(size) == 4:  # (N, C, H, W) 형식인 경우\n",
    "            W = size[2]\n",
    "            H = size[3]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected size format: {size}\")\n",
    "\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "        return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.dataset[index]\n",
    "\n",
    "        if np.random.rand() < self.prob:\n",
    "            lam = np.random.beta(self.beta, self.beta)\n",
    "            rand_index = np.random.randint(len(self.dataset))\n",
    "            img2, label2 = self.dataset[rand_index]\n",
    "\n",
    "            bbx1, bby1, bbx2, bby2 = self.rand_bbox(img1.size(), lam)\n",
    "            img1[:, bbx1:bbx2, bby1:bby2] = img2[:, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img1.size(-1) * img1.size(-2)))\n",
    "            label = lam * label1 + (1 - lam) * label2\n",
    "        else:\n",
    "            label = label1\n",
    "\n",
    "        return img1, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MDTA(nn.Module):\n",
    "    def __init__(self, channels, num_heads):\n",
    "        super(MDTA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
    "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n",
    "\n",
    "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
    "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
    "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
    "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
    "\n",
    "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
    "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
    "        return out\n",
    "\n",
    "\n",
    "class GDFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super(GDFN, self).__init__()\n",
    "\n",
    "        hidden_channels = int(channels * expansion_factor)\n",
    "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
    "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
    "                              groups=hidden_channels * 2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
    "        x = self.project_out(F.gelu(x1) * x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads, expansion_factor):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = MDTA(channels, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.ffn = GDFN(channels, expansion_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                          .contiguous().reshape(b, c, h, w))\n",
    "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                         .contiguous().reshape(b, c, h, w))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Restormer(nn.Module):\n",
    "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[24, 48, 96, 192], num_refinement=4, expansion_factor=2.66):\n",
    "        \n",
    "        super(Restormer, self).__init__()\n",
    "\n",
    "        self.embed_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
    "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
    "                                       zip(num_blocks, num_heads, channels)])\n",
    "        \n",
    "        # the number of down sample or up sample == the number of encoder - 1\n",
    "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
    "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
    "\n",
    "        # the number of reduce block == the number of decoder - 1\n",
    "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
    "                                      for i in reversed(range(2, len(channels)))])\n",
    "        \n",
    "        # the number of decoder == the number of encoder - 1\n",
    "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
    "                                                       for _ in range(num_blocks[2])])])\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
    "                                             for _ in range(num_blocks[1])]))\n",
    "        \n",
    "        # the channel of last one is not change\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                             for _ in range(num_blocks[0])]))\n",
    "\n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                          for _ in range(num_refinement)])\n",
    "        self.output = nn.Conv2d(channels[1], 3, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fo = self.embed_conv(x)\n",
    "        out_enc1 = self.encoders[0](fo)\n",
    "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))\n",
    "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))\n",
    "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))\n",
    "\n",
    "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))\n",
    "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))\n",
    "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))\n",
    "        fr = self.refinement(fd)\n",
    "        out = self.output(fr) + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CGBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_local = F.relu(self.bn1(self.conv1(x)))\n",
    "        x_local = F.relu(self.bn2(self.conv2(x_local)))\n",
    "        x_global = self.global_avg_pool(x_local)\n",
    "        x_global = self.fc(x_global)\n",
    "        return x_local + x_global\n",
    "\n",
    "class CGNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CGNet, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.cg_block1 = CGBlock(32, 64)\n",
    "        self.cg_block2 = CGBlock(64, 128)\n",
    "        self.classifier = nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.cg_block1(x)\n",
    "        x = self.cg_block2(x)\n",
    "        x = self.classifier(x)\n",
    "        return F.interpolate(x, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    '''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NAFBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(NAFBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)  # ReLU activation\n",
    "        x = self.conv3(x)\n",
    "        return shortcut + self.beta * x\n",
    "\n",
    "class NAFNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_blocks=16):\n",
    "        super(NAFNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[NAFBlock(64) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x + input\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 871/871 [20:20<00:00,  1.40s/batch, Loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Average Loss: 0.2196\n",
      "Model saved for epoch 1 with loss 0.2196\n",
      "Best model updated at epoch 1 with loss 0.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 871/871 [20:14<00:00,  1.39s/batch, Loss=0.0721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Average Loss: 0.2168\n",
      "Model saved for epoch 2 with loss 0.2168\n",
      "Best model updated at epoch 2 with loss 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 871/871 [20:32<00:00,  1.42s/batch, Loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Average Loss: 0.2167\n",
      "Model saved for epoch 3 with loss 0.2167\n",
      "Best model updated at epoch 3 with loss 0.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 871/871 [20:44<00:00,  1.43s/batch, Loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Average Loss: 0.2159\n",
      "Model saved for epoch 4 with loss 0.2159\n",
      "Best model updated at epoch 4 with loss 0.2159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 871/871 [20:45<00:00,  1.43s/batch, Loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Average Loss: 0.2153\n",
      "Model saved for epoch 5 with loss 0.2153\n",
      "Best model updated at epoch 5 with loss 0.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 871/871 [20:40<00:00,  1.42s/batch, Loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Average Loss: 0.2144\n",
      "Model saved for epoch 6 with loss 0.2144\n",
      "Best model updated at epoch 6 with loss 0.2144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 871/871 [20:33<00:00,  1.42s/batch, Loss=0.0333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Average Loss: 0.2148\n",
      "Model saved for epoch 7 with loss 0.2148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 871/871 [20:42<00:00,  1.43s/batch, Loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Average Loss: 0.2143\n",
      "Model saved for epoch 8 with loss 0.2143\n",
      "Best model updated at epoch 8 with loss 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 871/871 [20:37<00:00,  1.42s/batch, Loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Average Loss: 0.2142\n",
      "Model saved for epoch 9 with loss 0.2142\n",
      "Best model updated at epoch 9 with loss 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 871/871 [20:34<00:00,  1.42s/batch, Loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Average Loss: 0.2133\n",
      "Model saved for epoch 10 with loss 0.2133\n",
      "Best model updated at epoch 10 with loss 0.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 871/871 [20:42<00:00,  1.43s/batch, Loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Average Loss: 0.2141\n",
      "Model saved for epoch 11 with loss 0.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 871/871 [20:25<00:00,  1.41s/batch, Loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Average Loss: 0.2140\n",
      "Model saved for epoch 12 with loss 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 871/871 [20:13<00:00,  1.39s/batch, Loss=0.0464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Average Loss: 0.2119\n",
      "Model saved for epoch 13 with loss 0.2119\n",
      "Best model updated at epoch 13 with loss 0.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 871/871 [20:44<00:00,  1.43s/batch, Loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Average Loss: 0.2133\n",
      "Model saved for epoch 14 with loss 0.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 871/871 [20:33<00:00,  1.42s/batch, Loss=0.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Average Loss: 0.2146\n",
      "Model saved for epoch 15 with loss 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 데이터셋 경로\n",
    "noisy_image_paths = '/home/work/.default/hyunwoong/Contest/event/Training/noisy'\n",
    "clean_image_paths = '/home/work/.default/hyunwoong/Contest/event/Training/clean'\n",
    "\n",
    "# 데이터셋 및 DataLoader 설정\n",
    "train_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 커스텀 데이터셋 인스턴스 생성\n",
    "    train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
    "    cutmix_dataset = CutMixDataset(train_dataset, beta=1.0, prob=0.5)\n",
    "\n",
    "    # 데이터 로더 설정\n",
    "    train_loader = DataLoader(cutmix_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=0, shuffle=True)\n",
    "\n",
    "    # GPU 장치 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # NAFNet 모델 생성 및 GPU로 이동\n",
    "    model = NAFNet(in_channels=3, out_channels=3, num_blocks=8).to(device)\n",
    "\n",
    "    # 손실 함수와 최적화 알고리즘 설정\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "\n",
    "    # 학습 루프\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{CFG['EPOCHS']}\", unit=\"batch\") as pbar:\n",
    "            for noisy_images, clean_images in train_loader:\n",
    "                noisy_images = noisy_images.to(device)\n",
    "                clean_images = clean_images.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(noisy_images)\n",
    "                loss = criterion(outputs, clean_images)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                epoch_loss += loss.item() * noisy_images.size(0)\n",
    "                pbar.set_postfix({\"Loss\": loss.item()})\n",
    "                pbar.update(1)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # 각 epoch마다 모델 저장\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Model saved for epoch {epoch+1} with loss {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # 최적의 모델 저장\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            torch.save(model.state_dict(), 'best_NAFNet.pth')\n",
    "            print(f\"Best model updated at epoch {epoch+1} with loss {best_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from model_epoch_15.pth\n"
     ]
    }
   ],
   "source": [
    "# CustomDatasetTest 정의\n",
    "class CustomDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, transform=None):\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(noisy_image_path)\n",
    "\n",
    "        if isinstance(noisy_image, np.ndarray):\n",
    "            noisy_image = Image.fromarray(noisy_image)\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path\n",
    "\n",
    "# 이미지 로딩 함수\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "# NAFNet 모델 정의 및 가중치 불러오기\n",
    "model = NAFNet(in_channels=3, out_channels=3, num_blocks=8)\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 테스트 데이터 경로\n",
    "test_data_path = '/home/work/.default/hyunwoong/Contest/KMS_uni/open_(1)/test/Input'\n",
    "output_path = './open(1)/test/submission'\n",
    "test_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 테스트 데이터셋 및 데이터로더 설정\n",
    "test_dataset = CustomDatasetTest(test_data_path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 결과 저장 폴더 생성\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "def load_epoch_model(model, epoch):\n",
    "    model_path = f'model_epoch_{epoch}.pth'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "def denoise_and_save_images(model, test_loader, output_path):\n",
    "    with torch.no_grad():\n",
    "        for noisy_image, noisy_image_path in test_loader:\n",
    "            noisy_image = noisy_image.to(device)\n",
    "            denoised_image = model(noisy_image)\n",
    "            \n",
    "            # 후처리 및 저장\n",
    "            denoised_image = denoised_image.cpu().squeeze(0)\n",
    "            denoised_image = (denoised_image * 0.5 + 0.5).clamp(0, 1)\n",
    "            denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "            # 파일 경로 및 저장\n",
    "            output_filename = noisy_image_path[0]\n",
    "            denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.jpg'\n",
    "            denoised_image.save(denoised_filename) \n",
    "# 특정 epoch의 모델 가중치 로드 및 테스트셋에 적용\n",
    "epoch_to_load = 15  # 불러오고 싶은 epoch 번호 설정\n",
    "load_epoch_model(model, epoch_to_load)\n",
    "denoise_and_save_images(model, test_loader, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ./submission.zip successfully.\n"
     ]
    }
   ],
   "source": [
    "def zip_folder(folder_path, output_zip):\n",
    "    shutil.make_archive(output_zip, 'zip', folder_path)\n",
    "    print(f\"Created {output_zip}.zip successfully.\")\n",
    "\n",
    "zip_folder(output_path, './submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch181",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
